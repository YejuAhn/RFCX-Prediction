{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Description : \nhttps://www.notion.so/RFCX-9259c2261dda4a29a830e588e26e1b7e\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os,re,random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt  \nfrom pathlib import Path\nfrom IPython.display import Audio\nimport librosa\nimport librosa.display\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import StratifiedKFold\nimport io\nimport soundfile as sf\nfrom tqdm import tqdm\nimport cv2\nimport random\nfrom skimage.transform import resize\nimport tensorflow as tf\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nwith open('/kaggle/input/rfcx-species-audio-detection/train_tp.csv') as f:\n    TRAIN_TP = pd.read_csv(f)\nwith open('/kaggle/input/rfcx-species-audio-detection/train_fp.csv') as f:\n    TRAIN_FP = pd.read_csv(f)\nGCS_PATH = KaggleDatasets().get_gcs_path()\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/tfrecords/train/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/tfrecords/test/*.tfrec')\n\n\nprint('Train_TP size {} {}'.format(len(TRAIN_TP), len(TRAIN_TP.columns)))\nprint('Train_FP size {} {}'.format(len(TRAIN_FP), len(TRAIN_FP.columns)))\nprint('TRAINIG_Filenames size {}'.format(count_data_items(TRAINING_FILENAMES)))\nprint('TEST_Filenames size {}'.format(count_data_items(TEST_FILENAMES)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Parameters\n- Starter code from **Rainforest-Audio classification Tensorflow starter**[https://www.kaggle.com/dimitreoliveira/rainforest-audio-classification-tensorflow-starter#Spectrogram]"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16 * REPLICAS\nLEARNING_RATE = 1e-3 * REPLICAS\nEPOCHS = 15\nHEIGHT = 224\nWIDTH = 512\nCHANNELS = 3\nN_CLASSES = 24\nES_PATIENCE = 3\nTTA_STEPS = 6 # Do TTA if > 0 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper Functions\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_audio(audio_binary):\n    #decode a 16-bit PCM WAV file to a float tensor\n    audio, _ = tf.audio.decode_wav(audio_binary)\n    return tf.squeeze(audio, axis = -1)\n\ndef string_split_semicolon(column):\n    split_labels_sc = tf.strings.split(column, sep=';')\n    return split_labels_sc\n\ndef string_split_comma(column):\n    split_labels_c = tf.strings.split(column, sep=',')\n    return split_labels_c\n\ndef get_label_info(label_info):\n    first_split = string_split_semicolon(label_info)\n    remove_quotes = tf.strings.regex_replace(first_split, '\"\"', \"\")\n    label_info = string_split_comma(remove_quotes)\n    return label_info\n\n# all wave's sample rate = 48000 \ndef get_spectogram(waveform, padding = False, min_padding = 48000):\n    waveform = tf.cast(waveform, tf.float32)\n    if padding:\n        zero_padding = tf.zeros([min_padding] - tf.shape(waveform), dtype=tf.float32)\n        waveform = tf.concat([waveform, zero_padding], 0)\n    #short-time fourier transform of signals\n    spectrogram = tf.signal.stft(waveform, frame_length = 2048, frame_step=512, fft_length=2048)\n    spectrogram = tf.abs(spectrogram)\n    return spectrogram\n\ndef get_spectrogram_tf(example):\n    audio = example['audio_wav']\n    spectrogram = get_spectrogram(audio)\n    spectrogram = tf.expand_dims(spectrogram, -1)\n    example['audio_wav'] = spectrogram\n    return example   \n\ndef prepare_sample(example):\n    sample = example['audio_wav']\n    sample = tf.image.resize(sample, [HEIGHT, WIDTH])\n    sample = tf.image.grayscale_to_rgb(sample)\n    example['audio_wav'] = sample\n    return example\n\ndef crop_audio(audio, tmin, tmax, crop_size=10, sample_rate=48000, max_size=60):\n    label_size = tmax - tmin\n    #no padding needed\n    if label_size >= crop_size:\n        cut_min = tmin * sample_rate\n        cut_max = (tmin + crop_size) * sample_rate\n    else: \n        #needs padding\n        #pad at the end\n        if tmin <= (max_size - crop_size): \n            cut_min = tmin * sample_rate\n            cut_max = (tmin + crop_size) * sample_rate\n        else: \n            #pad at the beginning\n            cut_min = (tmin - crop_size) * sample_rate\n            cut_max = tmax * sample_rate\n    \n    cut_min = tf.cast(cut_min, tf.int32)\n    cut_max = tf.cast(cut_max, tf.int32)\n    cut_size = tf.cast((crop_size*sample_rate), tf.int32)\n    #croping the audio\n    audio = audio[cut_min:cut_max]\n    #making sure it has the max size\n    audio = audio[:cut_size]\n    #making sure it has the expected shape\n    audio = tf.reshape(audio, [cut_size]) \n    return audio\n\ndef random_crop_audio(audio, crop_size=10, sample_rate=48000, max_size=60):\n    start = tf.random.uniform([], minval=0, \n                              maxval=(max_size - crop_size), \n                              dtype=tf.int32)\n    cut_min = start * sample_rate\n    cut_max = (start + crop_size) * sample_rate\n    #casting tensors\n    cut_min = tf.cast(cut_min, tf.int32)\n    cut_max = tf.cast(cut_max, tf.int32)\n    cut_size = tf.cast((crop_size*sample_rate), tf.int32)\n    audio = audio[cut_min:cut_max] #croping the audio\n    audio = audio[:cut_size] #making sure it has the max size\n    audio = tf.reshape(audio, [cut_size])\n    return audio\n\n\"\"\"\n1. Parse data based on the 'TFREC_FORMAT' map.\n2. Decode PCM WAV file.\n3. Break down the information from 'label_info' into other features.\n4. Crop the 'audio' waveform if needed.\n5. Returns the features as a dictionary.\n\"\"\"\ndef read_tfrecord(example, labeled=True, inference=False):\n    TFREC_FORMAT = {\n        'audio_wav': tf.io.FixedLenFeature([], tf.string), \n        'recording_id': tf.io.FixedLenFeature([], tf.string), \n        'label_info': tf.io.FixedLenFeature([], tf.string, default_value='-1,-1,0,0,0,0,1'), \n    }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    audio = decode_audio(example['audio_wav'])\n    # Break down 'label_info' into the data columns\n    label_info = get_label_info(example['label_info'])\n    species_id = tf.strings.to_number(tf.gather_nd(label_info, [0, 0]), tf.int32)\n    tmin = tf.strings.to_number(tf.gather_nd(label_info, [0, 2]))\n    tmax = tf.strings.to_number(tf.gather_nd(label_info, [0, 4]))\n    is_tp = tf.strings.to_number(tf.gather_nd(label_info, [0, 6]), tf.int32)\n\n    if labeled:\n        audio = crop_audio(audio, tmin, tmax)\n    if inference:\n        audio = random_crop_audio(audio)\n        \n    features = {'audio_wav': audio, \n                'recording_id': example['recording_id'], \n                'species_id': species_id, \n                'is_tp': is_tp\n               }\n    return features\n\n#Load and parse the TFRecords\ndef load_dataset(filenames, labeled=True, ordered=False, inference=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n        dataset = tf.data.Dataset.list_files(filenames)\n        dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n    else:\n        dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)    \n    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled, inference=inference), num_parallel_calls=AUTO)\n    return dataset\n\n#Configure the output of the dataset\ndef conf_output(sample, labeled=True):\n    output = ({'input_audio': sample['audio_wav']}, sample['species_id'])\n    return output\n\n\"\"\"\n1. Load TFRecord files, parse and generate features (waveform and meta-data).\n2. Filter the dataset to contain only true positive samples.\n3. Create 'spectrogram' from the 'waveform'.\n4. Prepare image for the model.\n5. Configure data to have the expected output format.\n6. Apply Tensorflow data functions to optimize training.\nReturns a Tensorflow dataset ready for training or inference.\n\"\"\"\ndef get_dataset(filenames, labeled=True, ordered=False, repeated=False, inference=False):\n    dataset = load_dataset(filenames, labeled=labeled, inference=inference)   \n    if labeled:\n        dataset = dataset.filter(_filtterTP)\n    \n    dataset = dataset.map(get_spectrogram_tf, num_parallel_calls=AUTO)\n    dataset = dataset.map(prepare_sample, num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda x: conf_output(x, labeled=labeled), num_parallel_calls=AUTO)\n    \n    if not ordered:\n        dataset = dataset.shuffle(256)\n    if repeated:\n        dataset = dataset.repeat()\n        \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef _filtterTP(x):\n    return x['is_tp'] == 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization Helper Functions\ndef plot_spectrogram(spectrogram, ax):\n    #convert to frequencies to log scale and transpose so that the time is represented in the x-axis (columns).\n    log_spec = np.log(spectrogram.T)\n    height = log_spec.shape[0]\n    X = np.arange(spectrogram.shape[0])\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)\n    \ndef display_waveforms(ds, n_rows=3, n_cols=3, figsize=(20, 16)):\n    n = n_rows*n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    for i, sample in enumerate(ds.take(n)):\n        r = i // n_cols\n        c = i % n_cols\n        ax = axes[r][c]\n        ax.plot(sample['audio_wav'].numpy())\n        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n        label = sample['species_id'].numpy()\n        recording_id = sample['recording_id'].numpy().decode()\n        ax.set_title(f'{recording_id} - {label}')\n    plt.show()\n    \ndef display_spectrograms(ds, n_rows=3, n_cols=3, figsize=(20, 16)):\n    n = n_rows*n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    for i, sample in enumerate(ds.take(n)):\n        r = i // n_cols\n        c = i % n_cols\n        ax = axes[r][c]\n        plot_spectrogram(np.squeeze(sample['audio_wav'].numpy()), ax)\n        label = sample['species_id'].numpy()\n        recording_id = sample['recording_id'].numpy().decode()\n        ax.set_title(f'{recording_id} - {label}')\n    plt.show()\n    \ndef inspect_preds(features, labels, preds, n_rows=3, n_cols=2, figsize=(20, 16)):\n    n = n_rows*n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    for i, (feature, label, pred) in enumerate(zip(features, labels, preds)):\n        r = i // n_cols\n        c = i % n_cols\n        ax = axes[r][c]\n        plot_spectrogram(np.squeeze(feature), ax)\n        if pred == label:\n            color = 'black'\n            title = f'{pred} [True]'\n        else:\n            color = 'red'\n            title = f'{pred} [False, should be {label}]'\n        ax.set_title(title, fontsize=14, color=color)\n    plt.show()\n        \ndef display_waveforms_audio_spectrogram(ds, n_samples=1, sample_rate=48000):\n    for sample in ds.take(n_samples):\n        waveform = sample['audio_wav']\n        label = sample['species_id'].numpy()\n        recording_id = sample['recording_id'].numpy().decode()\n        spectrogram = get_spectrogram(waveform)\n\n        print(f'Name: {recording_id}')\n        print(f'Label: {label}')\n        print(f'Waveform shape: {waveform.shape}')\n        print(f'Spectrogram shape: {spectrogram.shape}')\n        print(f'Audio playback')\n        Idisplay.display(Idisplay.Audio(waveform, rate=sample_rate))\n        \n        fig, axes = plt.subplots(2, figsize=(12, 8))\n        timescale = np.arange(waveform.shape[0])\n        axes[0].plot(timescale, waveform.numpy())\n        axes[0].set_title('Waveform')\n        axes[0].set_xlim([0, waveform.shape[0]])\n        plot_spectrogram(spectrogram.numpy(), axes[1])\n        axes[1].set_title('Spectrogram')\n        plt.show()\n        \ndef inspect_preds(features, labels, preds, n_rows=3, n_cols=2, figsize=(20, 16)):\n    n = n_rows*n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    for i, (feature, label, pred) in enumerate(zip(features, labels, preds)):\n        r = i // n_cols\n        c = i % n_cols\n        ax = axes[r][c]\n        \n        feature = tf.image.rgb_to_grayscale(feature).numpy()\n        plot_spectrogram(np.squeeze(feature), ax)\n        if pred == label:\n            color = 'black'\n            title = f'{pred} [True]'\n        else:\n            color = 'red'\n            title = f'{pred} [False, should be {label}]'\n        ax.set_title(title, fontsize=14, color=color)\n    plt.show()\n    \n#model evaluation\ndef plot_metrics(history):\n    fig, axes = plt.subplots(2, 1, sharex='col', figsize=(20, 8))\n    axes = axes.flatten()\n    \n    axes[0].plot(history['loss'], label='Train loss')\n    axes[0].plot(history['val_loss'], label='Validation loss')\n    axes[0].legend(loc='best', fontsize=16)\n    axes[0].set_title('Loss')\n    axes[0].axvline(np.argmin(history['loss']), linestyle='dashed')\n    axes[0].axvline(np.argmin(history['val_loss']), linestyle='dashed', color='orange')\n    \n    axes[1].plot(history['sparse_categorical_accuracy'], label='Train accuracy')\n    axes[1].plot(history['val_sparse_categorical_accuracy'], label='Validation accuracy')\n    axes[1].legend(loc='best', fontsize=16)\n    axes[1].set_title('Accuracy')\n    axes[1].axvline(np.argmax(history['sparse_categorical_accuracy']), linestyle='dashed')\n    axes[1].axvline(np.argmax(history['val_sparse_categorical_accuracy']), linestyle='dashed', color='orange')\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_waveform_ds = load_dataset(TRAINING_FILENAMES)\n\ndisplay_waveforms(train_waveform_ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 2. Data Preprocessing - produce Mel Spectogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VAR\nfft = 2048\nhop = 512\nsr = 48000 # all wave's sampling rate may be 48k\nlength = 10 * sr\nfmin = 24000\nfmax = 0\nrng_seed = 1234\nrandom.seed(rng_seed)\nnp.random.seed(rng_seed)\nos.environ['PYTHONHASHSEED'] = str(rng_seed)\nkfold = 5\nnum_birds = 24\nlr = 0.04\nepochs = 24\n# Store data\nwith open('/kaggle/input/rfcx-species-audio-detection/train_tp.csv') as f:\n    reader = csv.reader(f)\n    TRAIN_DATA = list(reader)\nTRAIN_DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_min_max_frequency(data, fmin, fmax):\n    for i in range(1, len(data)):\n        if fmin > float(data[i][4]):\n            fmin = float(data[i][4])\n        if fmax < float(data[i][6]):\n            fmax = float(data[i][6])     \n    #saftey margin\n    fmin = int(fmin * 0.9)\n    fmax = int(fmax * 1.1)\n    print('Minimum frequency: ' + str(fmin) + ', maximum frequency: ' + str(fmax))\n    return(fmin, fmax)\n\ndef position_sound_slice(t_min, t_max, wav):\n    center = np.round((t_min + t_max) / 2)\n    beginning = center - length / 2\n    if beginning < 0:\n        beginning = 0\n\n    ending = beginning + length\n    if ending > len(wav):\n        ending = len(wav)\n        beginning = ending - length\n    return (beginning, ending, center)\n\ndef generate_mel_spectogram(data, fmin, fmax):  \n    for i in range(1, len(data)):\n        wav, sr = librosa.load('/kaggle/input/rfcx-species-audio-detection/train/' + data[i][0] + '.flac', sr=None)\n        t_min = float(data[i][3]) * sr\n        t_max = float(data[i][5]) * sr\n        beginning, ending, center = position_sound_slice(t_min, t_max, wav)\n        slice = wav[int(beginning):int(ending)]\n        \n        # Mel spectrogram generation\n        mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr, fmin=fmin, fmax=fmax, power=1.5)\n        mel_spec = resize(mel_spec, (224, 400))\n\n        # Normalize\n        mel_spec = mel_spec - np.min(mel_spec)\n        mel_spec = mel_spec / np.max(mel_spec)\n\n        mel_spec = mel_spec * 255\n        mel_spec = np.round(mel_spec)\n        mel_spec = mel_spec.astype('uint8')\n        mel_spec = np.asarray(mel_spec)\n\n        bmp = Image.fromarray(mel_spec, 'L')\n        # Saved as recording id_species_id_songtype_id format\n        bmp.save('/kaggle/working/' + data[i][0] + '_' + data[i][1] + '_' + str(center) + '.bmp')\n\n        if i % 100 == 0:\n            print('Processed ' + str(i) + ' train examples from ' + str(len(data)))\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fmin, fmax = get_min_max_frequency(TRAIN_DATA, fmin, fmax)\ngenerate_mel_spectogram(TRAIN_DATA, fmin, fmax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_file_label():\n    file_list = []\n    label_list = []\n    for f in os.listdir('/kaggle/working/'):\n        if '.bmp' in f:\n            file_list.append(f)\n            label = str.split(f, '_')[1]\n            label_list.append(label)\n    return(file_list, label_list)\n\ndef apply_stratified_kfold(file_list, label_list, kfold):\n    skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=rng_seed)\n    train_files = []\n    val_files = []\n    for fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n        # Picking only first fold to train/val on (loss of 20% training data)\n        if fold_id == 0:\n            train_files = np.take(file_list, train_index)\n            val_files = np.take(file_list, val_index)\n\n    print('Training on ' + str(len(train_files)) + ' examples')\n    print('Validating on ' + str(len(val_files)) + ' examples')\n    return (train_files, val_files)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_list, label_list = get_file_label()\ntrain_files, val_files = apply_stratified_kfold(file_list, label_list, kfold)\nprint(train_files, val_files)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Modeling - RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.utils.data as torchdata\n\nclass AudioForest():\n    def __init__(self, filelist):\n        self.specs = []\n        self.labels = []\n        for f in filelist:\n            label = int(str.split(f, '_')[1])\n            label_array = np.zeros(num_birds, dtype=np.single)\n            label_array[label] = 1.\n            self.labels.append(label_array)\n            \n            img = Image.open('/kaggle/working/' + f)\n            mel_spec = np.array(img)\n            img.close()\n            \n            mel_spec = mel_spec / 255\n            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n            self.specs.append(mel_spec)\n    \n    def __len__(self):\n        return len(self.specs)\n    \n    def __getitem__(self, item):\n        return self.specs[item], self.labels[item]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_list = []\nlabel_list = []\n\ndef get_files():\n    for f in os.listdir('/kaggle/working/'):\n        if '.bmp' in f:\n            file_list.append(f)\n            label = str.split(f, '_')[1]\n            label_list.append(label)\n    \nget_files()\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng_seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = []\nval_files = []\n\nfor fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n    if fold_id == 0:\n        train_files = np.take(file_list, train_index)\n        val_files = np.take(file_list, val_index)\n        \nprint('Training on ' + str(len(train_files)) + ' examples')\nprint('Validating on ' + str(len(val_files)) + ' examples')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\ntrain_dataset = AudioForest(train_files)\nval_dataset = AudioForest(val_files)\ntrain_loader = torchdata.DataLoader(train_dataset, batch_size=batch_size, sampler=torchdata.RandomSampler(train_dataset))\nval_loader = torchdata.DataLoader(val_dataset, batch_size=batch_size, sampler=torchdata.RandomSampler(val_dataset))\n\n\ndef create_model():\n    model = resnest50(pretrained=True)\n\n    model.fc = nn.Sequential(\n        nn.Linear(2048, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(1024, 24),\n    )\n\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import BCEWithLogitsLoss\n\nmodel = create_model()\ncriterion = BCEWithLogitsLoss() #PANNsLoss() #MaskedBCEWithLogitsLoss() #BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nnum_train_steps = int(len(train_loader) * epochs)\nnum_warmup_steps = int(0.1 * epochs * len(train_loader))\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.4)\n\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    loss_function = loss_function.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for e in range(0, 32):\n    # Stats\n    train_loss = []\n    train_corr = []\n    \n    model.train()\n    for batch, (data, target) in enumerate(train_loader):\n        data = data.float()\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Data Augmentation for wave form"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Base class for audio data transformation\nclass AudioTransform:\n    def __init__(self, always_apply=False, p=0.5):\n        self.always_apply = always_apply\n        self.p = p\n\n    def __call__(self, y: np.ndarray):\n        if self.always_apply:\n            return self.apply(y)\n        else:\n            if np.random.rand() < self.p:\n                return self.apply(y)\n            else:\n                return y\n\n    def apply(self, y: np.ndarray):\n        raise NotImplementedError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gaussian Noise\n\nclass AddGaussianNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_noise_amplitude=0.5, **kwargs):\n        super().__init__(always_apply, p)\n\n        self.noise_amplitude = (0.0, max_noise_amplitude)\n\n    def apply(self, y: np.ndarray, **params):\n        noise_amplitude = np.random.uniform(*self.noise_amplitude)\n        noise = np.random.randn(len(y))\n        augmented = (y + noise * noise_amplitude).astype(y.dtype)\n        return augmented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GaussianNoiseSNR(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        white_noise = np.random.randn(len(y))\n        a_white = np.sqrt(white_noise ** 2).max()\n        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n        return augmented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = AddGaussianNoise(always_apply=True, max_noise_amplitude=0.05)\ny_gaussian_added = transform(y)\nAudio(y_gaussian_added, rate=sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = GaussianNoiseSNR(always_apply=True, min_snr=5, max_snr=20)\ny_gaussian_snr = transform(y)\nAudio(y_gaussian_snr, rate=sr)\nlibrosa.display.waveplot(y_gaussian_snr, sr=sr);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melspec = librosa.power_to_db(librosa.feature.melspectrogram(y, sr=sr, n_mels=128))\nlibrosa.display.specshow(melspec, sr=sr, x_axis=\"time\", y_axis=\"mel\")\nplt.colorbar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import label_ranking_average_precision_score\ny_true = np.array([[1, 0, 0], [0, 0, 1]])\ny_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n\n\nlabel_ranking_average_precision_score(y_true, y_score)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv(\"../input/rfcx-species-audio-detection/sample_submission.csv\")\n\nss","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}